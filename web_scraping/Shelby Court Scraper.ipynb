{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO, BytesIO\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta, datetime\n",
    "import os\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from requests_base import RequestsPage\n",
    "\n",
    "\n",
    "#####Pages######\n",
    "##base\n",
    "class RequestsPage:\n",
    "        \n",
    "    def get_html(self, url, payload=None):\n",
    "        self.url = url\n",
    "        fakeuser = UserAgent()\n",
    "        headers = {'user-agent': fakeuser.chrome}\n",
    "        data = requests.get(url, headers=headers, params=payload)\n",
    "        #print(data.url)\n",
    "        return data\n",
    "\n",
    "    def save_html(self, data, filename):\n",
    "        self.file = open(filename, \"a\")\n",
    "        self.file.write(data)\n",
    "        self.file.close()\n",
    "        \n",
    "##config\n",
    "\n",
    "class ShelbyURLs:\n",
    "    #case_details = 'https://gscivildata.shelbycountytn.gov/pls/gnweb/ck_public_qry_doct.cp_dktrpt_docket_report?backto=C&case_id=2061459&begin_date=&end_date='\n",
    "    #case_details = 'https://gscivildata.shelbycountytn.gov/pls/gnweb/ck_public_qry_doct.cp_dktrpt_docket_report?backto=C&case_id={}&begin_date=&end_date='\n",
    "    case_details = 'https://gscivildata.shelbycountytn.gov/pls/gnweb/ck_public_qry_doct.cp_dktrpt_docket_report?'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoDateDiscoveryMixIn():\n",
    "    \n",
    "    \n",
    "    def _get_today_date(self):\n",
    "        self.current_date = datetime.today().date()\n",
    "        \n",
    "    \n",
    "    def _clean_date(self, date):\n",
    "        self.date = date[0].split(\", \")\n",
    "        self.final_date = f'{self.date[2].strip()}-{self.date[1].split()[0]}-{self.date[1].split()[1].strip()[0:2]}'\n",
    "        self.clean_date = datetime.strptime(self.final_date, '%Y-%B-%d').date()\n",
    "        return(self.clean_date)\n",
    "\n",
    "  \n",
    "    def _create_tree(self):\n",
    "        self.parser = etree.HTMLParser()\n",
    "        self.tree = etree.parse(StringIO(self.output.text), self.parser)\n",
    "    \n",
    "    def _locators(self):\n",
    "        #self.case_present = self.tree.xpath('//*[contains(text(), \"No case was found\")]/text()')\n",
    "        #self.date = self._clean_date(self.tree.xpath('//table[2]//tr[2]//td[3]//text()'))\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    def _search_no_case(self):\n",
    "        \n",
    "        self.case_present = self.tree.xpath(self.case_status)\n",
    "        ls_dates = []\n",
    "        counter = 0   \n",
    "        while len(self.case_present) == False:\n",
    "            counter = counter +1\n",
    "            date = self._clean_date(self.tree.xpath(self.filing_date))\n",
    "            \n",
    "            daily_average_filings = 40\n",
    "\n",
    "            if counter != 1:\n",
    "                latest_date = ls_dates.pop()\n",
    "                if ((date-latest_date).days)<0:\n",
    "                    date = latest_date\n",
    "                    \n",
    "            ls_dates.append(date)\n",
    "            days_between = (self.current_date - date).days\n",
    "\n",
    "            if days_between < 10:\n",
    "                daily_average_filings = 10\n",
    "            if days_between == 0:\n",
    "                days_between = 1\n",
    "\n",
    "            case_increment = days_between * daily_average_filings\n",
    "            self.case_number = self.case_number + case_increment\n",
    "            self.page_source(\"Shelby\", self.case_number)\n",
    "            self._create_tree()\n",
    "            self.case_present = self.tree.xpath(self.case_status)\n",
    "\n",
    "           \n",
    "        #pass\n",
    "\n",
    "    def _search_for_case(self):\n",
    "        \n",
    "        while len(self.case_present) == True:\n",
    "            self.case_number = self.case_number-1\n",
    "            self.page_source(\"Shelby\", self.case_number)\n",
    "            self._create_tree()\n",
    "            self.case_present = self.tree.xpath(self.case_status)\n",
    "            \n",
    "    def manuall_site_param(self):\n",
    "        raise DumbDeveloperException('This function requires a manual script. Are you a developer? Did you forget to overwrite manual_site_parm()')\n",
    "    \n",
    "        \n",
    "            \n",
    "                \n",
    "   \n",
    "    def most_recent_case(self, start_case, filing_date, case_status, driver=None):\n",
    "        \n",
    "        self.filing_date = filing_date\n",
    "        self.case_status = case_status\n",
    "        self.start_case = start_case\n",
    "        self._get_today_date()\n",
    "        self.page_source(\"Shelby\", self.start_case)\n",
    "        self._create_tree()\n",
    "        self._search_no_case()\n",
    "        self._search_for_case()\n",
    "        print(f'most recent case is: {self.case_number}')\n",
    "        return(self.case_number)\n",
    "       \n",
    "        #self.clean_date = self._clean_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##search_page\n",
    "class SearchPage(RequestsPage, NoDateDiscoveryMixIn):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _build_payload(self):\n",
    "        self.payload = {\n",
    "            'backto' : \"C\",\n",
    "            'case_id':  self.case_number,\n",
    "            'begin_date': '',\n",
    "            'end_date' : ''\n",
    "                       }\n",
    "    \n",
    "    \n",
    "    def page_source(self, county, case_number):\n",
    "        self.county = county\n",
    "        \n",
    "        self.case_number = case_number\n",
    "        self._build_payload()\n",
    "        self.url = ShelbyURLs.case_details.format(self.case_number)\n",
    "        \n",
    "        self.output = self.get_html(self.url, payload = self.payload)\n",
    "        #print(self.url)\n",
    "        #print(\"+++\")\n",
    "        #print(self.output.url)\n",
    "        \n",
    "        #self.save_html(self.output, \"test2\")\n",
    "        \n",
    "    def most_recent_case(self, start_case):\n",
    "        super().most_recent_case(start_case ,'//table[2]//tr[2]//td[3]//text()','//*[contains(text(), \"No case was found\")]/text()' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.lxml_parser = etree.HTMLParser()\n",
    "\n",
    "    \n",
    "    def open_html(self, html_file):\n",
    "        self.html_file = html_file\n",
    "        self.html = open(self.html_file, \"r\")\n",
    "        self.html = self.html.read()\n",
    "        #lxml will not access text after <br> tags and xpaths to their location return nothing, thus removal solves the problem with no known drawbacks\n",
    "        self.html = self.html.replace('<BR>', ' ')\n",
    "        #print(self.html)\n",
    "        return etree.parse(StringIO(self.html), self.lxml_parser)\n",
    "    \n",
    "    def _get_info(self, location):\n",
    "        self.location = location\n",
    "        #print(\"+++++++\")\n",
    "        #print(self.tree.xpath(f'{self.location}//text()'))\n",
    "        #print(\"++++++++\")\n",
    "        return self.tree.xpath(self.location)\n",
    "    \"\"\"\n",
    "    def _get_info2(self, location):\n",
    "        self.location = location\n",
    "        #print(\"+++++++\")\n",
    "        #print(self.tree.xpath(f'{self.location}//text()'))\n",
    "        #print(\"++++++++\")\n",
    "        return self.tree.xpath(f'{self.location}//text()')\n",
    "    \"\"\"\n",
    "    \n",
    "    def clean(self, dirty):\n",
    "        self.dirty = dirty\n",
    "        self.dirty = self.dirty.lower().strip().replace('\\n', '')\n",
    "        return self.dirty\n",
    "\n",
    "    def find_element_by_xpath(self, location):\n",
    "        self.info = self._get_info(location)\n",
    "        if isinstance(self.info, list):\n",
    "            if len(self.info) == 0:\n",
    "                #print('length is 0')\n",
    "                return 'none'\n",
    "            elif self.info[0].text != None and self.clean(self.info[0].text) != '':\n",
    "                #print('position 1 is not none')\n",
    "                return self.clean(self.info[0].text)\n",
    "            else:\n",
    "                #print('should be returning string of none')\n",
    "                return 'none'\n",
    "        elif self.info == None:\n",
    "            #print('not a list, is none')\n",
    "            return 'none'\n",
    "        else:\n",
    "            #print('not a list, not none')\n",
    "            return self.info\n",
    "    \n",
    "    def find_elements_by_xpath(self, location):\n",
    "        self.info = self._get_info(location)\n",
    "        return self.info\n",
    "        if isinstance(self.info, list):\n",
    "            if len(self.info) == 1:\n",
    "                raise ElementException('Element only returned one element. Try find_element_by_xpath (singular)')\n",
    "            else:\n",
    "                return self.info\n",
    "        else:\n",
    "            raise ElementException('Element only returned one element. Try find_element_by_xpath (singular).')\n",
    "    \n",
    "    #counts elements returned by find_elements_by_xpath, returns number\n",
    "    def get_length_by_xpath(self, location):\n",
    "        self.info = self._get_info(location)\n",
    "        return len(self.info)\n",
    "    \n",
    "    # this function writes multiples 'none's to a given list until that list is 3 items long\n",
    "    def expand_list(self, list, length):\n",
    "        self.list = list\n",
    "        self.length = length\n",
    "        while len(self.list) < self.length:\n",
    "            try:\n",
    "                self.list.append('none')\n",
    "            except:\n",
    "                pass\n",
    "        return self.list\n",
    "\n",
    "    def more_than(self, list, count):\n",
    "        self.list = list\n",
    "        self.count = count\n",
    "        if len(self.list) > self.count:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "        \n",
    "    def text_between(self, text, before, after):\n",
    "        self.text = text\n",
    "        self.before = before\n",
    "        self.after = after\n",
    "        try:\n",
    "            self.parse = self.text.split(self.before)\n",
    "            self.parse = self.parse[1].split(self.after)\n",
    "            self.parse = self.clean(self.parse[0])\n",
    "        except:\n",
    "            self.parse = 'none'\n",
    "        return self.parse\n",
    "\n",
    "    def does_string_appear(self, text, string):\n",
    "        self.text = text\n",
    "        self.string = string\n",
    "        self.test = self.text.find(self.string)\n",
    "        if self.test != -1:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    \n",
    "    #this function provides an entry_point to the best xpath logic for new users\n",
    "    #it follows this logic{div/table item is in}/{div/html element text is in}/{text to search}/{xpath to follow to get to relative field}\n",
    "    def string_search(self, text, table, route=None, text_location=None):\n",
    "        self.text = text\n",
    "        self.table = table\n",
    "        self.route = route\n",
    "        self.text_location = text_location\n",
    "        if self.text_location == None and self.route == None:\n",
    "            #('no location and no route')\n",
    "            return self.find_element_by_xpath(f'{self.table}//*[contains(text(), \"{text}\")]')\n",
    "        elif self.text_location == None and self.route != None:\n",
    "            #('no location')\n",
    "            return self.find_element_by_xpath(f'{self.table}//*[contains(text(), \"{text}\")]/{self.route}')\n",
    "        elif self.text_location != None and self.route == None:\n",
    "            #('no route')\n",
    "            return self.find_element_by_xpath(f'{self.table}//{self.text_location}[contains(text(), \"{text}\")]')\n",
    "        elif self.text_location != None and self.route != None:\n",
    "            #('location and route')\n",
    "            return self.find_element_by_xpath(f'{self.table}//{self.text_location}[contains(text(), \"{text}\")]/{self.route}')\n",
    "    \n",
    "    \n",
    "    # this function creates a new database and failure log but wont overwrite existing one\n",
    "    def create_csv(self, database_name, header):\n",
    "        self.database_name = database_name\n",
    "        self.header = header\n",
    "        self.file = f'{file_path}/{self.database_name}'\n",
    "        if path.exists(self.file):\n",
    "            print('CSV already exists.')\n",
    "        else:\n",
    "            with open(self.file, 'w', newline='') as self.outfile:\n",
    "                self.writer = csv.writer(self.outfile)\n",
    "                self.writer.writerow(self.header)\n",
    "            print('New CSV created.')\n",
    "    \n",
    "    def write_data(self, data_out, database_name):\n",
    "        #writing data to csv\n",
    "        self.data_out = data_out\n",
    "        self.database_name = database_name\n",
    "        self.file = f'{file_path}/{self.database_name}'\n",
    "        with open(self.file, 'a', newline='') as self.outfile:\n",
    "            self.writer = csv.writer(self.outfile, delimiter=',')\n",
    "            self.writer.writerow(self.data_out)\n",
    "            \n",
    "    def write_json_data(self, data_out, database_name):\n",
    "        #writing data to json\n",
    "        self.data_out = data_out\n",
    "        self.database_name = database_name\n",
    "        self.file = f'{file_path}/{self.database_name}'\n",
    "        with open(self.file, 'w') as self.outfile:\n",
    "            json.dump(self.data_out, self.outfile)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShelbyParser(Parser):\n",
    "    \n",
    "    \n",
    "        \n",
    "    def shelby_eviction(self, tree):\n",
    "        self.tree = tree\n",
    "        \n",
    "        #Initialize storage containers\n",
    "        self.case_object = dict()\n",
    "        self.party_obj = []\n",
    "        self.docket_obj = []\n",
    "        \n",
    "        ##table locations\n",
    "        self.case_description_table = '//*[contains(text(), \"Case Description\")]/parent::node()/parent::node()//following-sibling::table[1]'\n",
    "        self.report_selection_criteria_table = '//*[contains(text(), \"Report Selection Criteria\")]/parent::node()/parent::node()//following-sibling::table[1]' \n",
    "        self.case_parties_table = '//*[contains(text(), \"Case Parties\")]/parent::node()//following-sibling::table[1]'\n",
    "        self.docket_entries_table = '//*[contains(text(), \"Docket Entries\")]/parent::node()/parent::node()//following-sibling::table[1]'\n",
    "        self.get_horizontal_info = '/parent::node()//following-sibling::td[1]'\n",
    "        self.rows_route = 'descendant::tr'\n",
    "        \n",
    "        ##Horizontal information\n",
    "        #print(f'Horizontal Info -----------------------------------------------------------------------')\n",
    "        self.case_id = self.string_search(\"Case ID\", self.report_selection_criteria_table, route = self.get_horizontal_info, text_location = 'b')\n",
    "        self.filing_date = self.string_search(\"Filing Date\", self.case_description_table, route = self.get_horizontal_info, text_location = 'b')\n",
    "        self.case_type = self.string_search(\"Type\", self.case_description_table, route = self.get_horizontal_info, text_location = 'b')\n",
    "        self.status = self.string_search(\"Status\", self.case_description_table, route = self.get_horizontal_info, text_location = 'b')\n",
    "        \n",
    "        #print(f'Case: {self.case_id} and {self.type}')\n",
    "        ##Vertical Information\n",
    "        \n",
    "        \n",
    "        ####Case Parties\n",
    "        #print(f'Case Partieeeeeees -----------------------------------------------------------------------')\n",
    "        self.case_parties_row_locator = f'{self.case_parties_table}/{self.rows_route}'        \n",
    "        self.rows = self.get_length_by_xpath(f'{self.case_parties_row_locator}')\n",
    "                \n",
    "        self.info_rows_index = list(range(1,self.rows))\n",
    "        self.info_row_index_grouped = list(zip(*[iter(self.info_rows_index)]*3))\n",
    "        \n",
    "        \n",
    "        for index in self.info_row_index_grouped:\n",
    "            self.index = index  \n",
    "            #plus 1 to account for first row being headers\n",
    "            self.first_row = index[0]+1\n",
    "            self.second_row = index[1]+1\n",
    "            \n",
    "    \n",
    "            self.xpath1 = f'{self.case_parties_row_locator}[{self.first_row}]/td'\n",
    "            self.xpath2 =  f'{self.case_parties_row_locator}[{self.second_row}]/td'\n",
    "\n",
    "\n",
    "            self.seq = self.find_element_by_xpath(f'{self.xpath1}[1]')\n",
    "            self.assoc = self.find_element_by_xpath(f'{self.xpath1}[2]')\n",
    "            self.expn_date = self.find_element_by_xpath(f'{self.xpath1}[3]')\n",
    "            self.type = self.find_element_by_xpath(f'{self.xpath1}[4]')\n",
    "            self.id = self.find_element_by_xpath(f'{self.xpath1}[5]')\n",
    "            self.parties_name = self.find_element_by_xpath(f'{self.xpath1}[6]')\n",
    "            \n",
    "            self.address = self.find_element_by_xpath(f'{self.xpath2}[2]')\n",
    "            self.aliases = self.find_element_by_xpath(f'{self.xpath2}[4]')\n",
    "            \n",
    "            self.party_dict = {\"seq\":self.seq , \"assoc\":self.assoc,\"expnDate\":self.expn_date,\"type\":self.type,\"id\":self.id ,\"name\":self.parties_name, \"address\":self.address,\"aliases\" :self.aliases } \n",
    "            self.party_obj.append(self.party_dict)\n",
    "\n",
    "        \n",
    "\n",
    "        ####Docket Entries\n",
    "       # print(f'Docket Entrieeeessss -----------------------------------------------------------------------')\n",
    "        self.docket_entries_row_locator = f'{self.docket_entries_table}/{self.rows_route}'        \n",
    "        self.rows = self.get_length_by_xpath(f'{self.docket_entries_row_locator}')\n",
    "                \n",
    "        self.info_rows_index = list(range(1,self.rows))\n",
    "        self.info_row_index_grouped = list(zip(*[iter(self.info_rows_index)]*3))\n",
    "        \n",
    "        \n",
    "        for index in self.info_row_index_grouped:\n",
    "            self.index = index  \n",
    "            #plus 1 to account for first row being headers\n",
    "            self.first_row = index[0]+1\n",
    "            self.second_row = index[1]+1\n",
    "            \n",
    "    \n",
    "            self.xpath1 = f'{self.docket_entries_row_locator}[{self.first_row}]/td'\n",
    "            self.xpath2 =  f'{self.docket_entries_row_locator}[{self.second_row}]/td'\n",
    "\n",
    "\n",
    "            self.filingDate = self.find_element_by_xpath(f'{self.xpath1}[1]')\n",
    "            self.description = self.find_element_by_xpath(f'{self.xpath1}[2]')\n",
    "            self.docket_name = self.find_element_by_xpath(f'{self.xpath1}[3]')\n",
    "            self.monetary = self.find_element_by_xpath(f'{self.xpath1}[4]')\n",
    "            \n",
    "            \n",
    "            self.entry = self.find_element_by_xpath(f'{self.xpath2}[2]')\n",
    "            \n",
    "            \n",
    "            self.docket_dict = {\"filingDate\": self.filingDate,  \"description\":self.description, \"name\": self.docket_name, \"monetary\": self.monetary, \"entry\":self.entry}\n",
    "\n",
    "            self.docket_obj.append(self.docket_dict)\n",
    "            \n",
    "        #Compile final data\n",
    "        self.case_object[\"case_id\"] = self.case_id        \n",
    "        self.case_object[\"filing_date\"] = self.filing_date\n",
    "        self.case_object[\"type\"] = self.case_type\n",
    "        self.case_object[\"status\"] = self.status\n",
    "        self.case_object[\"case_parties\"] = self.party_obj\n",
    "        self.case_object[\"docket_entries\"] = self.docket_obj\n",
    "        \n",
    "        #print(self.case_object)\n",
    "        return(self.case_object)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This searches for the most recent case number when given a previously exisiting case number\n",
    "search = SearchPage()\n",
    "search.most_recent_case(2061459)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_case = 2074248\n",
    "ls_urls = []\n",
    "for case_number in range(start_case,last_case+1):\n",
    "    url = f'https://gscivildata.shelbycountytn.gov/pls/gnweb/ck_public_qry_doct.cp_dktrpt_docket_report?backto=C&case_id={case_number}&begin_date=&end_date='\n",
    "    ls_urls.append(url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load documents into array after they are downloaded\n",
    "\n",
    "parse = ShelbyParser()\n",
    "\n",
    "ls_html = []\n",
    "path=\"C:/Users/nicho/Documents/Code/howard-center-webscrapers2/Shelby/\" \n",
    "\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    ls_html.append(parse.open_html(f'{path}/{file}'))\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out parsed court cases into json\n",
    "caseArr = []\n",
    "\n",
    "count = 0\n",
    "for case in ls_html:\n",
    "    count = count + 1\n",
    "\n",
    "    result = parse.shelby_eviction(case)\n",
    "    caseArr.append(result)\n",
    "    #clear_output(wait=True)\n",
    "    print(count)\n",
    "\n",
    "parse.write_json_data(caseArr, \"shelby2020.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
